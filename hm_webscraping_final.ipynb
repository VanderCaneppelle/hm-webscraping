{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0753828a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T01:08:51.117171Z",
     "start_time": "2022-07-07T00:55:27.847081Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime   import datetime\n",
    "from bs4        import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#### DATA COLLECTION#####\n",
    "def data_collection(url,headers):\n",
    "     # Entrando home page, para ver e número total de produtos e fazer a paginação\n",
    "\n",
    "     page = requests.get (url, headers = headers)\n",
    "     soup = BeautifulSoup(page.text , 'html.parser')\n",
    "\n",
    "     total_items = soup.find_all('h2', class_ = 'load-more-heading')[0].get('data-total')\n",
    "\n",
    "     page_number = np.ceil(int(total_items)/36)\n",
    "\n",
    "     #===== Adicionando paginação =========\n",
    "     url2 = url + '?page-size=' + str(int(page_number*36))\n",
    "\n",
    "     headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n",
    "     page2 = requests.get (url2, headers = headers)\n",
    "\n",
    "     # beutifull soap object\n",
    "     soup2 = BeautifulSoup(page2.text , 'html.parser')\n",
    "\n",
    "     #lista de produtos\n",
    "     products = soup2.find('ul', class_ = 'products-listing')\n",
    "\n",
    "     #========= Coleta de dados ===========\n",
    "     #product_id and product_category\n",
    "     product_list = products.find_all('article', class_ = 'hm-product-item')\n",
    "     product_id = [p.get('data-articlecode')for p in product_list]\n",
    "\n",
    "     product_cat = [p.get('data-category')for p in product_list]\n",
    "\n",
    "     # product_name\n",
    "     product_list = products.find_all('a', class_ = 'link')\n",
    "     product_name = [p.get_text('link')for p in product_list]\n",
    "\n",
    "     #product_price\n",
    "     product_list = products.find_all('span', class_ = 'price regular')\n",
    "     product_price = [p.get_text().replace('$ ','').strip() for p in product_list]\n",
    "\n",
    "     # product_datetime\n",
    "     data = pd.DataFrame([product_id,product_cat,product_name,product_price]).T\n",
    "     data.columns = ['product_id','product_cat','product_name','product_price']\n",
    "\n",
    "     data['scrapy_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "     return data\n",
    "\n",
    "\n",
    " # empty dataframe\n",
    " ##### DATA COLLECTION BY PRODUCT#####\n",
    "def data_collection_by_product(data,headers):\n",
    "     df_compositions = pd.DataFrame()\n",
    "\n",
    "     # unique columns for all products\n",
    "     aux = []\n",
    "\n",
    "     df_pattern = pd.DataFrame(columns=['Fit','Art. No.', 'Composition'])\n",
    "     for i in range(len(data)):\n",
    "         # API Requests\n",
    "         url = 'https://www2.hm.com/en_us/productpage.' + str(data.loc[i,'product_id']) + '.html'\n",
    "         logger.debug('Product: %s', url)\n",
    "\n",
    "         page = requests.get(url, headers = headers)\n",
    "\n",
    "         # BeautifulSoup Objetc criation\n",
    "         soup = BeautifulSoup(page.text , 'html.parser')\n",
    "\n",
    "      # ================color name ==========================\n",
    "         product_list = soup.find_all('a', class_ ='filter-option miniature active') + soup.find_all('a', class_ ='filter-option miniature')\n",
    "         color_name = [p.get('data-color') for p in product_list]\n",
    "         product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "         df_color = pd.DataFrame([product_id,color_name]).T\n",
    "         df_color.columns = ['product_id','color_name']\n",
    "\n",
    "         for j in range( len(df_color)):\n",
    "\n",
    "             try:\n",
    "             # API Requests\n",
    "                 url = 'https://www2.hm.com/en_us/productpage.' + str(df_color.loc[j,'product_id']) + '.html'\n",
    "                 logger.debug('Product: %s', url)\n",
    "     #\n",
    "             # request\n",
    "                 page = requests.get (url, headers = headers)\n",
    "\n",
    "             # BeautifulSoup Objetc criation\n",
    "                 soup = BeautifulSoup(page.text , 'html.parser')\n",
    "\n",
    "             #product_name\n",
    "                 product_list = soup.find_all('h1')\n",
    "                 product_name = [p.get_text('h1')for p in product_list][0]\n",
    "\n",
    "             #product_price\n",
    "                 product_list = soup.find_all('div', class_ = 'primary-row product-item-price')\n",
    "                 product_price = [p.get_text().strip().replace('$','') for p in product_list][0]\n",
    "\n",
    "             #        size\n",
    "                 product_list = soup.find_all('dl')\n",
    "                 product_size = [p.get_text().split('\\n')[3] for p in product_list]\n",
    "                 product_size = [(product_size)[0].strip()][0]\n",
    "\n",
    "             #         ================composition==========================\n",
    "                 product_list = soup.find_all('div', class_ ='details-attributes-list-item')\n",
    "                 product_composition = [list(filter(None, p.get_text().split('\\n'))) for p in product_list]\n",
    "\n",
    "             #        # criando DataFrame\n",
    "                 df_composition = pd.DataFrame(product_composition).T\n",
    "\n",
    "             #         # Renomeando os valores das colunas, de acordo com a primeira linha.\n",
    "                 df_composition.columns = df_composition.iloc[0]\n",
    "\n",
    "             #        # dropando a primeira linha\n",
    "                 df_composition = df_composition.iloc[1:].fillna(method='pad')\n",
    "                 df_composition = df_composition.drop_duplicates(subset=None,keep='first', inplace = False, ignore_index=False)\n",
    "\n",
    "                 df_composition['Composition'] = df_composition['Composition'].replace( 'Pocket lining: ','', regex=True)\n",
    "                 df_composition['Composition'] = df_composition['Composition'].replace( 'Shell: ','', regex=True)\n",
    "                 df_composition['Composition'] = df_composition['Composition'].replace( 'Lining: ','', regex=True)\n",
    "                 df_composition['Composition'] = df_composition['Composition'].replace( 'Pocket: ','', regex=True)\n",
    "\n",
    "\n",
    "             # garantee the same number of columns\n",
    "                 df_composition =  pd.concat( [df_pattern, df_composition], axis = 0)\n",
    "                 df_composition = df_composition[['Art. No.','Composition','Fit']]\n",
    "\n",
    "             #    Rename colums\n",
    "                 df_composition.columns = ['product_id','composition','fit']\n",
    "\n",
    "             # adding new columns\n",
    "                 df_composition['product_name'] = product_name\n",
    "                 df_composition['product_price'] = product_price\n",
    "                 df_composition['product_size'] = product_size\n",
    "\n",
    "             #         #keeps new columns if it shows up\n",
    "                 aux = aux + df_composition.columns.tolist()\n",
    "\n",
    "             #         # merge color and composition\n",
    "                 df_composition = pd.merge(df_composition, df_color, how='left', on='product_id')\n",
    "             #\n",
    "             # all products\n",
    "                 df_compositions = pd.concat( [df_compositions, df_composition], axis=0 )\n",
    "                  # generate style_id and color_id (separando product_id para obter o numero do style_id e o color_id)\n",
    "             except(IndexError):\n",
    "                 pass\n",
    "\n",
    "     df_compositions['style_id'] = df_compositions['product_id'].apply(lambda x: x[:-3] )\n",
    "     df_compositions['color_id'] = df_compositions['product_id'].apply(lambda x: x[-3:] )\n",
    "\n",
    "     # # #Scrapy Date\n",
    "     df_compositions['scrapy_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "     return df_compositions\n",
    "\n",
    " ##### DATA CLEANING #####\n",
    "def data_cleaning(data_product):\n",
    "     # product_id - ok\n",
    "     df_data = data_product.dropna(subset=['product_id'])\n",
    "\n",
    "     # product_name\n",
    "     df_data['product_name'] = df_data['product_name'].apply(lambda x: x.replace(' ','_').lower())\n",
    "\n",
    "     # product_price  - ok\n",
    "     df_data['product_price'] = df_data['product_price'].astype(float)\n",
    "\n",
    "     # color_name\n",
    "     df_data['color_name'] = df_data['color_name'].apply(lambda x: x.replace(' ','_').replace('/','_').lower() if pd.notnull(x) else x )\n",
    "     # Fit\n",
    "     df_data['fit'] = df_data['fit'].apply(lambda x: x.replace(' ','_').lower() if pd.notnull(x) else x )\n",
    "\n",
    "     # Size number\n",
    "     # df_data['size_number']=df_data['product_size'].apply(lambda x:re.search('\\d{3}cm',x).group(0) if pd.notnull(x) else x)\n",
    "     df_data['size_model']=df_data['product_size'].str.split(' ', expand=True)[3]\n",
    "     df_data['size_model']=df_data['product_size'].str.extract('(\\d{3})cm')\n",
    "\n",
    "     #size_model\n",
    "     df_data['size_number']=df_data['product_size'].str.extract('(\\d+/\\\\d+)')\n",
    "\n",
    "     # break compositon by comma\n",
    "     df1 = df_data['composition'].str.split(',',  expand=True).reset_index(drop=True)\n",
    "\n",
    "     # #columns order = cotton | Polyester | Elastomultiester | Spandex\n",
    "     df_ref = pd.DataFrame( index = np.arange(len( data)), columns= ['cotton', 'polyester','elastomultiester','spandex'])\n",
    "\n",
    "     # # =============== Composition =========================\n",
    "     # ====== cotton ======\n",
    "     df_cotton_0 = df1.loc[ df1[0].str.contains('Cotton', na=True), 0]\n",
    "     df_cotton_0.name ='cotton'\n",
    "     df_cotton_1 = df1.loc[ df1[1].str.contains('Cotton', na=True), 1]\n",
    "     df_cotton_1.name ='cotton'\n",
    "     df_cotton = df_cotton_0.combine_first( df_cotton_1 )\n",
    "     df_ref = pd.concat([df_ref, df_cotton], axis=1)\n",
    "     df_ref = df_ref.iloc[:,~df_ref.columns.duplicated(keep = 'last')]\n",
    "\n",
    "     # ======== polyester===========\n",
    "     df_polyester_0 = df1.loc[df1[0].str.contains('Polyester ', na=True),0]\n",
    "     df_polyester_0.name = 'polyester'\n",
    "     df_polyester_1 = df1.loc[df1[1].str.contains('Polyester ', na=True),1]\n",
    "     df_polyester_1.name = 'polyester'\n",
    "     df_polyester = df_polyester_0.combine_first( df_polyester_1 )\n",
    "     df_ref = pd.concat([df_ref, df_polyester],axis =1)\n",
    "     df_ref = df_ref.iloc[:,~df_ref.columns.duplicated(keep = 'last')]\n",
    "\n",
    "     # ================Spandex ===============\n",
    "     df_spandex_1 = df1.loc[df1[1].str.contains('Spandex', na=True),1]\n",
    "     df_spandex_1.name = 'spandex'\n",
    "     df_spandex_2 = df1.loc[df1[2].str.contains('Spandex', na=True),2]\n",
    "     df_spandex_2.name = 'spandex'\n",
    "\n",
    "     # # Combine spandex from both columns 1 e 2\n",
    "     df_spandex = df_spandex_1.combine_first( df_spandex_2 )\n",
    "\n",
    "     df_ref = pd.concat([df_ref, df_spandex],axis =1)\n",
    "     df_ref = df_ref.iloc[:,~df_ref.columns.duplicated(keep = 'last')]\n",
    "\n",
    "     # ============= Elastomultiester ===========\n",
    "     df_elastomultiester = df1.loc[df1[1].str.contains('Elastomultiester', na=True),1]\n",
    "     df_elastomultiester.name = 'elastomultiester'\n",
    "     df_ref = pd.concat([df_ref, df_elastomultiester],axis =1)\n",
    "     df_ref = df_ref.iloc[:,~df_ref.columns.duplicated(keep = 'last')]\n",
    "\n",
    "     # join  of combine  with product_id\n",
    "     df_aux = pd.concat([df_data['product_id'].reset_index(drop=True),df_ref], axis=1 )\n",
    "\n",
    "     # #remove word from composition\n",
    "     df_aux['cotton'] = df_aux['cotton'].apply(lambda x : int(re.search('\\d+', x ).group(0))/ 100 if pd.notnull(x) else x )\n",
    "     df_aux['polyester'] = df_aux['polyester'].apply(lambda x : int(re.search('\\d+', x ).group(0))/ 100 if pd.notnull(x) else x )\n",
    "     df_aux['elastomultiester'] = df_aux['elastomultiester'].apply(lambda x : int(re.search('\\d+', x ).group(0))/ 100 if pd.notnull(x) else x )\n",
    "     df_aux['spandex'] = df_aux['spandex'].apply(lambda x : int(re.search('\\d+', x ).group(0))/ 100 if pd.notnull(x) else x )\n",
    "\n",
    "     # final join\n",
    "     df_aux = df_aux.groupby ( 'product_id' ).max().reset_index().fillna(0)\n",
    "     df_data = pd.merge(df_data, df_aux, on='product_id', how='left')\n",
    "\n",
    "     # #droping columns unneeded\n",
    "     df_data =df_data.drop(columns = ['composition','product_size'])\n",
    "\n",
    "     df_data = df_data.drop_duplicates().reset_index(drop=True)\n",
    "     return df_data\n",
    "\n",
    " ##### DATA INSERT #####\n",
    "def data_insert( df_data ):\n",
    "     # ## Data Insert - DataBase\n",
    "     data_insert = df_data[[\n",
    "         'product_id',\n",
    "         'style_id',\n",
    "         'color_id',\n",
    "         'product_name',\n",
    "         'color_name',\n",
    "         'fit',\n",
    "         'product_price',\n",
    "         'size_number',\n",
    "         'size_model',\n",
    "         'cotton',\n",
    "         'polyester',\n",
    "         'spandex',\n",
    "         'elastomultiester',\n",
    "         'scrapy_datetime'\n",
    "     ]]\n",
    "\n",
    "     #create database connection\n",
    "     conn =create_engine ('sqlite:///database_hm.sqlite', echo=False)\n",
    "\n",
    "     #data insert\n",
    "     data_insert.to_sql('vitrine', con=conn, if_exists='append', index=False)\n",
    "     return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     #logging\n",
    "     path = 'C:\\\\Users\\\\vande\\\\CienciaDeDados\\\\DS_ao_DEV\\\\Module_06'\n",
    "\n",
    "     if not os.path.exists(path + 'Logs'):\n",
    "         os.makedirs(path + 'Logs')\n",
    "\n",
    "     logging.basicConfig(\n",
    "         filename=path + 'Logs\\webscraping_hm.log',\n",
    "         level=logging.DEBUG,\n",
    "         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "         datefmt='%Y-%m-%d %H:%M:%S'\n",
    "     )\n",
    "     logger = logging.getLogger('webscraping_hm')\n",
    "\n",
    "     #parameters and constants\n",
    "     headers = {\n",
    "         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n",
    "\n",
    "     url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "     #data collection\n",
    "     data = data_collection(url,headers)\n",
    "     logger.info('data collection done')\n",
    "     #data collection by product\n",
    "     data_product = data_collection_by_product(data,headers)\n",
    "     logger.info('data collection by product done')\n",
    "\n",
    "     #data cleaning\n",
    "     data_product_cleaned = data_cleaning(data_product)\n",
    "     logger.info('data product cleaned done')\n",
    "\n",
    "#     #insertion\n",
    "     data_insert(data_product_cleaned)\n",
    "     logger.info('data insertion done')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5ce62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
